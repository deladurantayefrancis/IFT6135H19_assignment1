{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import gzip, pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = gzip.open('mnist.pkl.gz')\n",
    "data = pickle.load(f, encoding='latin1')\n",
    "\n",
    "train_data = data[0][0]\n",
    "valid_data, valid_labels = data[1]\n",
    "test_data, test_labels = data[2]\n",
    "\n",
    "# onehot encoding for valid and test labels\n",
    "train_labels = np.zeros((train_data.shape[0], 10))\n",
    "train_labels[np.arange(train_labels.shape[0]), data[0][1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    \n",
    "    def __init__(self, hidden_dims=(1024,2048), n_hidden=2, mode='train', datapath=None, model_path=None,\n",
    "                batchsize=64, lr=0.0001, delta=.5, activation=\"ReLU\", initialization=\"normal\", epsilon=1e-9):\n",
    "        \n",
    "        self.dims = (784,) + hidden_dims + (10,)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.mode = mode\n",
    "        self.datapath = datapath\n",
    "        self.model_path = model_path\n",
    "        self.batchsize = batchsize\n",
    "        self.lr = lr\n",
    "        self.delta = delta\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # set activation function for hidden layers\n",
    "        if activation == \"ReLU\":\n",
    "            self.activation = self.ReLU\n",
    "            self.activation_prime = self.ReLU_prime\n",
    "        elif activation == \"sigmoid\":\n",
    "            self.activation = self.sigmoid\n",
    "            self.activation_prime = self.sigmoid_prime\n",
    "        elif activation == \"tanh\":\n",
    "            self.activation = self.tanh\n",
    "            self.activation_prime = self.tanh_prime\n",
    "        else:\n",
    "            raise ValueError('Invalid activation function specified: ' + str(activation))\n",
    "        \n",
    "        # network parameters\n",
    "        self.W = []\n",
    "        self.b = [np.zeros(self.dims[i]) for i in range(1, len(self.dims))]\n",
    "        \n",
    "        # weight initialization\n",
    "        self.initialize_weights(n_hidden, self.dims, initialization)\n",
    "        \n",
    "        # keep count of completed training epochs\n",
    "        self.epoch_cnt = 1\n",
    "    \n",
    "    ############################\n",
    "    #  Weight initializations  #\n",
    "    ############################\n",
    "    \n",
    "    def initialize_weights(self, n_hidden, dims, option):\n",
    "        \n",
    "        if option == \"zero\":\n",
    "            for i in range(n_hidden + 1):\n",
    "                self.W.append( np.zeros((dims[i+1], dims[i])) )\n",
    "        \n",
    "        elif (option == \"normal\"):\n",
    "            for i in range(n_hidden + 1):\n",
    "                self.W.append( np.random.normal(0, 1, (dims[i+1], dims[i])) )\n",
    "        \n",
    "        elif (option == \"glorot\"):\n",
    "            for i in range(n_hidden + 1):\n",
    "                di = np.sqrt(6/(dims[i]+dims[i+1]))\n",
    "                self.W.append( np.random.uniform(-di, di, (dims[i+1], dims[i])) )\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Invalid weight initialization specified: ' + str(option))\n",
    "    \n",
    "    #################################################\n",
    "    #  Activation functions  and their derivatives  #\n",
    "    #################################################\n",
    "    \n",
    "    def ReLU(self, input):\n",
    "        return np.maximum(0, input)\n",
    "    \n",
    "    def ReLU_prime(self, input):\n",
    "        return np.where(input > 0, 1, 0)\n",
    "    \n",
    "    def sigmoid(self, input):\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "    \n",
    "    def sigmoid_prime(self, input):\n",
    "        return self.sigmoid(input) * (1 - self.sigmoid(input))\n",
    "    \n",
    "    def tanh(self, input):\n",
    "        return 2 / (1 + np.exp(-2*input)) - 1\n",
    "    \n",
    "    def tanh_prime(self, input):\n",
    "        return 1 - self.tanh(input)**2\n",
    "    \n",
    "    def softmax(self, input):\n",
    "        rescaled = input - np.amax(input, axis=1)[:,np.newaxis] # for numerical stability\n",
    "        input_exp = np.exp(rescaled)\n",
    "        return input_exp / np.sum(input_exp, axis=1)[:,np.newaxis]\n",
    "    \n",
    "    ########################################\n",
    "    #  Forward, backward, loss and update  #\n",
    "    ########################################\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        # cache stores z = Wx + b of every layer for backprop (z=x for first layer)\n",
    "        cache = [input]\n",
    "        \n",
    "        out = input\n",
    "        for i in range(self.n_hidden):\n",
    "            out = out @ self.W[i].T + self.b[i] # compute z = Wx + b\n",
    "            cache.append(out) # store z for backprop\n",
    "            out = self.activation(out) # compute a = activation(z)\n",
    "        \n",
    "        out = out @ self.W[-1].T + self.b[-1]\n",
    "        cache.append(out)\n",
    "        out = self.softmax(out)\n",
    "        \n",
    "        return out, cache\n",
    "    \n",
    "    def loss(self, prediction, labels):\n",
    "        return -np.sum(labels * np.log(prediction + self.epsilon)) / self.batchsize\n",
    "    \n",
    "    def backward(self, cache, labels):\n",
    "        \n",
    "        # cache contains z = Wx + b of every layer (z=x for first layer)\n",
    "        \n",
    "        grads_a = [0] * (self.n_hidden + 1)\n",
    "        \n",
    "        grads_a[-1] = self.softmax(cache[-1]) - labels\n",
    "        for i in range(self.n_hidden-1, -1, -1):\n",
    "            grads_a[i] = grads_a[i+1] @ self.W[i+1] * self.activation_prime(cache[i+1])\n",
    "        \n",
    "        grads_W = [0] * (self.n_hidden + 1)\n",
    "        grads_b = [0] * (self.n_hidden + 1)\n",
    "        \n",
    "        for i in range(self.n_hidden, -1, -1):\n",
    "            grads_W[i] = grads_a[i].T @ self.activation(cache[i]) / self.batchsize\n",
    "            grads_b[i] = np.mean(grads_a[i], axis=0)\n",
    "            \n",
    "        return (grads_W, grads_b)\n",
    "    \n",
    "    \n",
    "    def update(self, grads, lr):\n",
    "        for i in range(self.n_hidden + 1):\n",
    "            self.W[i] -= lr * grads[0][i]\n",
    "            self.b[i] -= lr * grads[1][i]\n",
    "        \n",
    "    ##############################\n",
    "    #  Train and test functions  #\n",
    "    ##############################\n",
    "    \n",
    "    def train(self, train_data, train_labels, n_epochs):\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            print(datetime.now(), \"-\", \"Epoch\", self.epoch_cnt, end=\": \") \n",
    "            \n",
    "            # learning rate for this epoch (doesn't change during first 5 epochs)\n",
    "            t = max(1, self.epoch_cnt-5)\n",
    "            lr = self.lr / t**self.delta\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for i in range(train_data.shape[0] // self.batchsize):\n",
    "            \n",
    "                # start and end of mini-batch\n",
    "                start = i * self.batchsize\n",
    "                end = (i+1) * self.batchsize\n",
    "                \n",
    "                # forward pass and epoch loss\n",
    "                predictions, cache = self.forward(train_data[start:end])\n",
    "                epoch_loss += self.loss(predictions, train_labels[start:end])\n",
    "                \n",
    "                # backward pass and update\n",
    "                grads = self.backward(cache, train_labels[start:end])\n",
    "                self.update(grads, lr)\n",
    "            \n",
    "            print(\"loss =\", epoch_loss)\n",
    "            self.epoch_cnt += 1\n",
    "        \n",
    "        # self.finite_diff()\n",
    "        \n",
    "        \n",
    "    def test(self, test_data, test_labels):\n",
    "        \n",
    "        predictions, cache = self.forward(test_data)\n",
    "        successes = (np.argmax(predictions, axis=1) == test_labels)\n",
    "        \n",
    "        print(str(100 * np.sum(successes) / test_data.shape[0]) + \"% success\")\n",
    "    \n",
    "    ############################################\n",
    "    #  Finite difference computation funciton  #\n",
    "    ############################################\n",
    "    \n",
    "    def finite_diff(self, sample, label):\n",
    "        random.seed(9001)\n",
    "        #first layer weight\n",
    "        weight_save = np.copy(self.W[1])\n",
    "        \n",
    "        #choose five random value from set N\n",
    "        num_N = 5\n",
    "        P = np.min([10, hidden_dims[1]])\n",
    "        w_size = 256\n",
    "        N = []\n",
    "        K = [1, 5]\n",
    "        fi_grad = np.zeros((num_N, P, w_size))\n",
    "        for num_N in range(num_N):\n",
    "            i = random.randint(0,5)\n",
    "            k = K[random.randint(0,1)]\n",
    "            N.append(k * (10**i))\n",
    "        \n",
    "        N.sort()\n",
    "        print(N)\n",
    "        #calculate finite difference for first 10 weights in first layer\n",
    "        for i in range(len(N)):\n",
    "            epsilon = 1 / N[i]\n",
    "            print(\"Set Epsilon = \", epsilon)\n",
    "            grad = np.zeros((P,w_size))\n",
    "            for p in range(P):\n",
    "                #restore the original weight before going to next finite difference calculation\n",
    "                self.W[1] = np.copy(weight_save)\n",
    "                for z in range(w_size):\n",
    "                    w1 = np.copy(self.W[1])\n",
    "                    w2 = np.copy(self.W[1])\n",
    "\n",
    "                    w1[p][z] -= epsilon\n",
    "                    w2[p][z] += epsilon\n",
    "\n",
    "                    #theta(i) - epsilon\n",
    "                    self.W[1] = np.copy(w1)\n",
    "                    L1, dum = self.forward(sample)\n",
    "                    L1_loss = self.loss(L1, label)\n",
    "                    #theta(i) + epsilon\n",
    "                    self.W[1] = np.copy(w2)\n",
    "                    L2, dum = self.forward(sample)\n",
    "                    L2_loss = self.loss(L2, label)\n",
    "                    g = (L2_loss-L1_loss)/(2*epsilon)\n",
    "                    grad[p,z] = g\n",
    "                    \n",
    "                    \n",
    "            fi_grad[i,:] = grad\n",
    "            \n",
    "        self.W[1] = np.copy(weight_save)\n",
    "        #True gradient Calculation\n",
    "        prediction, cache = self.forward(sample)\n",
    "        loss = self.loss(prediction, label)\n",
    "        grads = self.backward(cache, label)\n",
    "        \n",
    "        #fi_grad(finite_difference)\n",
    "        \n",
    "        return N, fi_grad, grads[0][1][0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dims = (256, 2048)\n",
    "n_hidden = 2\n",
    "mode = 'train'\n",
    "datapath = None\n",
    "model_path = None\n",
    "batchsize = 32\n",
    "lr = .01\n",
    "delta = .5\n",
    "activation = \"sigmoid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying with glorot initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6135)\n",
    "net_glorot = NN(hidden_dims, n_hidden, mode, datapath, model_path,\n",
    "                batchsize, lr, delta, activation, initialization=\"glorot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-11 13:45:25.914358 - Epoch 1: loss = 3569.9555987238964\n",
      "30.32% success\n",
      "2019-02-11 13:45:39.873295 - Epoch 2: loss = 3263.523134930674\n",
      "57.21% success\n",
      "2019-02-11 13:45:53.370384 - Epoch 3: loss = 2664.6264730668645\n",
      "64.98% success\n",
      "2019-02-11 13:46:07.169520 - Epoch 4: loss = 1956.3737065367163\n",
      "70.13% success\n",
      "2019-02-11 13:46:20.742740 - Epoch 5: loss = 1485.194446435469\n",
      "76.76% success\n",
      "2019-02-11 13:46:34.283123 - Epoch 6: loss = 1213.7158886150455\n",
      "80.49% success\n",
      "2019-02-11 13:46:47.950787 - Epoch 7: loss = 1058.685302479545\n",
      "83.61% success\n",
      "2019-02-11 13:47:01.472951 - Epoch 8: loss = 977.520159704424\n",
      "84.57% success\n",
      "2019-02-11 13:47:15.177574 - Epoch 9: loss = 924.0104470025773\n",
      "85.41% success\n",
      "2019-02-11 13:47:28.757628 - Epoch 10: loss = 884.8193694388555\n",
      "85.86% success\n",
      "2019-02-11 13:47:42.248953 - Epoch 11: loss = 854.2567317080496\n",
      "86.17% success\n",
      "2019-02-11 13:47:55.746655 - Epoch 12: loss = 829.4035974569988\n",
      "86.63% success\n",
      "2019-02-11 13:48:09.281695 - Epoch 13: loss = 808.5827750134108\n",
      "86.93% success\n",
      "2019-02-11 13:48:22.890443 - Epoch 14: loss = 790.7507674828828\n",
      "87.23% success\n",
      "2019-02-11 13:48:36.484661 - Epoch 15: loss = 775.218151970732\n",
      "87.41% success\n",
      "2019-02-11 13:48:50.088736 - Epoch 16: loss = 761.5073118731402\n",
      "87.69% success\n",
      "2019-02-11 13:49:03.652977 - Epoch 17: loss = 749.2743607087207\n",
      "87.82% success\n",
      "2019-02-11 13:49:17.212689 - Epoch 18: loss = 738.2636617296339\n",
      "88.0% success\n",
      "2019-02-11 13:49:30.726133 - Epoch 19: loss = 728.2800138916846\n",
      "88.15% success\n",
      "2019-02-11 13:49:44.305378 - Epoch 20: loss = 719.1709340098378\n",
      "88.27% success\n",
      "2019-02-11 13:49:57.810155 - Epoch 21: loss = 710.8149700294689\n",
      "88.37% success\n",
      "2019-02-11 13:50:11.392623 - Epoch 22: loss = 703.1137556464025\n",
      "88.48% success\n",
      "2019-02-11 13:50:24.922424 - Epoch 23: loss = 695.986462970384\n",
      "88.61% success\n",
      "2019-02-11 13:50:38.635212 - Epoch 24: loss = 689.3658369853056\n",
      "88.72% success\n",
      "2019-02-11 13:50:52.144975 - Epoch 25: loss = 683.1953003198206\n",
      "88.95% success\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    net_glorot.train(train_data, train_labels, 1)\n",
    "    net_glorot.test(valid_data, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.74% success\n"
     ]
    }
   ],
   "source": [
    "net_glorot.test(train_data, data[0][1])\n",
    "# net_glorot.test(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying with normal initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6135)\n",
    "net_normal = NN(hidden_dims, n_hidden, mode, datapath, model_path,\n",
    "                batchsize, lr, delta, activation, initialization=\"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-11 13:51:08.050338 - Epoch 1: loss = 7865.612901551834\n",
      "74.33% success\n",
      "2019-02-11 13:51:21.212898 - Epoch 2: loss = 3549.3301700791767\n",
      "79.96% success\n",
      "2019-02-11 13:51:34.879429 - Epoch 3: loss = 2706.8724582193113\n",
      "81.96% success\n",
      "2019-02-11 13:51:48.531626 - Epoch 4: loss = 2263.4461377864745\n",
      "83.43% success\n",
      "2019-02-11 13:52:02.244016 - Epoch 5: loss = 1969.6895710217789\n",
      "84.15% success\n",
      "2019-02-11 13:52:16.043214 - Epoch 6: loss = 1751.5099465231397\n",
      "84.69% success\n",
      "2019-02-11 13:52:29.816065 - Epoch 7: loss = 1572.9959022005712\n",
      "85.14% success\n",
      "2019-02-11 13:52:43.686251 - Epoch 8: loss = 1465.0835358445177\n",
      "85.37% success\n",
      "2019-02-11 13:52:57.384956 - Epoch 9: loss = 1387.6496580577525\n",
      "85.65% success\n",
      "2019-02-11 13:53:11.196962 - Epoch 10: loss = 1327.146343968756\n",
      "85.93% success\n",
      "2019-02-11 13:53:26.151738 - Epoch 11: "
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    net_normal.train(train_data, train_labels, 1)\n",
    "    net_normal.test(valid_data, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_normal.test(train_data, data[0][1])\n",
    "# net_normal.test(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying with zero initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6135)\n",
    "net_zero = NN(hidden_dims, n_hidden, mode, datapath, model_path,\n",
    "              batchsize, lr, delta, activation, initialization=\"zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    net_zero.train(train_data, train_labels, 1)\n",
    "    net_zero.test(valid_data, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_zero.test(train_data, data[0][1])\n",
    "# net_zero.test(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finite difference gradient for first 10 weights in first layer\n",
    "NN, f_grad, t_grad = net_glorot.finite_diff(np.expand_dims(train_data[0],axis=0), np.expand_dims(train_labels[0],axis=0))\n",
    "print (NN)\n",
    "\n",
    "#calculate maximum difference\n",
    "fgd = []\n",
    "for i in range(len(NN)):\n",
    "    euclidean = np.sum((t_grad-f_grad[i])**2, axis=1)\n",
    "    fgd.append(np.max(euclidean))\n",
    "\n",
    "#plot the maximum difference\n",
    "plt.figure()\n",
    "plt.plot(NN, np.log(fgd))\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('Log of maximum difference')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
