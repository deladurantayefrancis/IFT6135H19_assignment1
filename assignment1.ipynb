{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import gzip, pickle\n",
    "import numpy as np\n",
    "\n",
    "f = gzip.open('mnist.pkl.gz')\n",
    "data = pickle.load(f, encoding='latin1')\n",
    "\n",
    "train_data = data[0][0]\n",
    "valid_data, valid_labels = data[1]\n",
    "test_data, test_labels = data[2]\n",
    "\n",
    "# onehot encoding for valid and test labels\n",
    "train_labels = np.zeros((train_data.shape[0], 10))\n",
    "train_labels[np.arange(train_labels.shape[0]), data[0][1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    \n",
    "    def __init__(self, hidden_dims=(1024,2048), n_hidden=2, mode='train', datapath=None, model_path=None,\n",
    "                batchsize=64, lr=0.001, epsilon=1e-8, activation=\"ReLU\", initialization=\"normal\"):\n",
    "        \n",
    "        self.dims = (784,) + hidden_dims + (10,)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.mode = mode\n",
    "        self.datapath = datapath\n",
    "        self.model_path = model_path\n",
    "        self.batchsize = batchsize\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # set activation function for hidden layers\n",
    "        if activation == \"ReLU\":\n",
    "            self.activation = self.ReLU\n",
    "            self.activation_prime = self.ReLU_prime\n",
    "        else:\n",
    "            raise ValueError('Invalid activation function specified: ' + str(activation))\n",
    "        \n",
    "        # network parameters\n",
    "        self.W = []\n",
    "        self.b = [np.zeros(self.dims[i]) for i in range(1, len(self.dims))]\n",
    "        \n",
    "        # weight initialization\n",
    "        self.initialize_weights(n_hidden, self.dims, initialization)\n",
    "        \n",
    "        # keep count of completed training epochs\n",
    "        self.epoch_cnt = 0\n",
    "        \n",
    "    \n",
    "    def initialize_weights(self, n_hidden, dims, option):\n",
    "        \n",
    "        if option == \"zero\":\n",
    "            for i in range(n_hidden + 1):\n",
    "                self.W.append( np.zeros((dims[i+1], dims[i])) )\n",
    "        \n",
    "        elif (option == \"normal\"):\n",
    "            for i in range(n_hidden + 1):\n",
    "                self.W.append( np.random.normal(0, 1, (dims[i+1], dims[i])) )\n",
    "        \n",
    "        elif (option == \"glorot\"):\n",
    "            for i in range(n_hidden + 1):\n",
    "                di = (6/(dims[i]+dims[i+1]))**.5\n",
    "                self.W.append( np.random.uniform(-di, di, (dims[i+1], dims[i])) )\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Invalid weight initialization specified: ' + str(option))\n",
    "    \n",
    "    \n",
    "    def forward(self, input): # missing params\n",
    "        \n",
    "        cache = [input] # to store z = Wx + b of every layer for backprop\n",
    "        \n",
    "        out = input\n",
    "        for i in range(self.n_hidden):\n",
    "            out = self.W[i] @ out + self.b[i] # compute z = Wx + b\n",
    "            cache.append(out) # store z for backprop\n",
    "            out = self.activation(out) # compute a = activation(z)\n",
    "        \n",
    "        out = self.W[-1] @ out + self.b[-1]\n",
    "        cache.append(out)\n",
    "        out = self.softmax(out)\n",
    "        \n",
    "        return out, cache\n",
    "    \n",
    "    \n",
    "    def ReLU(self, input):\n",
    "        return np.maximum(0, input)\n",
    "    \n",
    "    def ReLU_prime(self, input):\n",
    "        return np.where(input > 0, 1, 0)\n",
    "    \n",
    "    def loss(self, prediction, labels):\n",
    "        return -np.sum(labels * np.log(prediction + self.epsilon)) / self.batchsize\n",
    "    \n",
    "    def softmax(self, input): # missing params\n",
    "        rescaled = input - np.amax(input) # for numerical stability\n",
    "        input_exp = np.exp(rescaled)\n",
    "        return input_exp / np.sum(input_exp)\n",
    "    \n",
    "    def backward(self, cache, labels): # missing params\n",
    "        \n",
    "        # cache contains z = Wx + b of every layer (including first layer where z = x)\n",
    "        grads_a = [0] * (self.n_hidden + 1)\n",
    "        \n",
    "        grads_a[-1] = self.softmax(cache[-1]) - labels\n",
    "        for i in range(self.n_hidden-1, -1, -1):\n",
    "            grads_a[i] = grads_a[i+1] @ self.W[i+1] * self.activation_prime(cache[i+1])\n",
    "        \n",
    "        grads_W = [0] * (self.n_hidden + 1)\n",
    "        grads_b = [0] * (self.n_hidden + 1)\n",
    "        \n",
    "        for i in range(self.n_hidden, -1, -1):\n",
    "            grads_W[i] = np.outer(grads_a[i], self.activation(cache[i]))\n",
    "            grads_b[i] = grads_a[i]\n",
    "            \n",
    "        return (grads_W, grads_b)\n",
    "    \n",
    "    \n",
    "    def update(self, grads): # missing params\n",
    "        for i in range(self.n_hidden + 1):\n",
    "            self.W[i] -= self.lr * grads[0][i]\n",
    "            self.b[i] -= self.lr * grads[1][i]\n",
    "    \n",
    "    \n",
    "    def train(self, train_data, train_labels, n_epochs):\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            print(datetime.now(), \"-\", \"Epoch\", self.epoch_cnt+1, end=\": \") \n",
    "            \n",
    "            loss = 0\n",
    "            for i in range(train_data.shape[0]):\n",
    "                predictions, cache = self.forward(train_data[i])\n",
    "                loss += self.loss(predictions, train_labels[i])\n",
    "                \n",
    "                grads = self.backward(cache, train_labels[i])\n",
    "                self.update(grads)\n",
    "            \n",
    "            print(\"loss =\", loss)\n",
    "            self.epoch_cnt += 1\n",
    "        \n",
    "    \n",
    "    def test(self, test_data, test_labels):\n",
    "        \n",
    "        successes = 0\n",
    "        \n",
    "        for i in range(test_data.shape[0]):\n",
    "            predictions, cache = self.forward(test_data[i])\n",
    "            if np.argmax(predictions) == test_labels[i]:\n",
    "                successes += 1\n",
    "        \n",
    "        print(str(100 * successes / test_data.shape[0]) + \"% success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying with glorot initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dims=(1024,2048)\n",
    "n_hidden=2\n",
    "mode='train'\n",
    "datapath=None\n",
    "model_path=None\n",
    "\n",
    "net = NN(hidden_dims, n_hidden, mode, datapath, model_path, batchsize=1, initialization=\"glorot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-07 22:53:39.107752 - Epoch 1: loss = 1915.7651581240034\n",
      "2019-02-07 22:53:55.054655 - Epoch 2: loss = 1171.0766697432662\n",
      "2019-02-07 22:54:12.910620 - Epoch 3: loss = 770.7325424963356\n",
      "2019-02-07 22:54:28.707283 - Epoch 4: loss = 585.710522215171\n",
      "2019-02-07 22:54:42.326545 - Epoch 5: loss = 482.77374841147065\n"
     ]
    }
   ],
   "source": [
    "net.train(train_data[:1000], train_labels[:1000], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.2% success\n"
     ]
    }
   ],
   "source": [
    "net.test(test_data[:1000], test_labels[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying with normal initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dims=(1024,2048)\n",
    "n_hidden=2\n",
    "mode='train'\n",
    "datapath=None\n",
    "model_path=None\n",
    "\n",
    "net = NN(hidden_dims, n_hidden, mode, datapath, model_path, batchsize=1, initialization=\"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-07 22:54:58.866517 - Epoch 1: loss = 5839.356130496439\n",
      "2019-02-07 22:55:11.313612 - Epoch 2: loss = 2118.3681307209254\n",
      "2019-02-07 22:55:24.182870 - Epoch 3: loss = 828.9305198418704\n",
      "2019-02-07 22:55:37.891680 - Epoch 4: loss = 725.0176876910268\n",
      "2019-02-07 22:55:50.996748 - Epoch 5: loss = 782.8087994230723\n"
     ]
    }
   ],
   "source": [
    "net.train(train_data[:1000], train_labels[:1000], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.1% success\n"
     ]
    }
   ],
   "source": [
    "net.test(test_data[:1000], test_labels[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
